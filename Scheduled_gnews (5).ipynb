{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw3t3CeVN9NW"
      },
      "outputs": [],
      "source": [
        "!pip install GoogleNews\n",
        "!pip install gnewsclient\n",
        "!pip install snscrape==0.6.2.20230320\n",
        "!pip install rake_nltk\n",
        "\n",
        "import pandas as pd\n",
        "# from newsapi import NewsApiClient\n",
        "import requests\n",
        "from GoogleNews import GoogleNews\n",
        "from gnewsclient import gnewsclient\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "from datetime import datetime, date\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rake_nltk import Rake\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "id425SVcN9Nb"
      },
      "outputs": [],
      "source": [
        "start_date = []\n",
        "end_date = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9z79OMANN9Nc"
      },
      "outputs": [],
      "source": [
        "sources = [\"bbc-news\", \"the-telegraph\", \"the-guardian-uk\", \"cnn\", \"abc-news-au\",\n",
        "           \"dailymail.co.uk\", \"metro.co.uk\", \"mirror.co.uk\", \"news.google.com\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bgr77j2AN9Nd"
      },
      "outputs": [],
      "source": [
        "all_keywords = ['strike', 'holiday', 'lockdown',\n",
        "            'inflation', 'grocery sales', 'carnival', 'festival', 'party', 'Walmart', \"Tesco\", \"Sainsbury's\", \"supply chain\", \"flood\", \"wendys\", \"lidl\"]\n",
        "\n",
        "# all_keywords = ['tesco', 'holiday']\n",
        "\n",
        "# all_keywords = ['autumn', 'bank']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Pqjh5tBuy7hC"
      },
      "outputs": [],
      "source": [
        "keywords = ['Lidl','Tesco','Walmart','Sainsbury\\'s', 'Aldi', 'Asda', 'Marks & Spencers', 'Morrison\\'s']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "98VGmhUlrzSq"
      },
      "outputs": [],
      "source": [
        "events = ['autumn bank holiday']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UlDJce6V8jka"
      },
      "outputs": [],
      "source": [
        "all_events = ['autumn bank holiday']\n",
        "\n",
        "final_prod_events = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cKZSWp8ZWckH"
      },
      "outputs": [],
      "source": [
        "counter = 6000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Etrp7MlCN9Ne"
      },
      "outputs": [],
      "source": [
        "gnews_client_topics = ['Top Stories',\n",
        "                       'World',\n",
        "                       'Nation',\n",
        "                       'Business',\n",
        "                       'Technology',\n",
        "                       'Entertainment',\n",
        "                       'Sports',\n",
        "                       'Science',\n",
        "                       'Health']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aiYeWUcQUhqR"
      },
      "outputs": [],
      "source": [
        "# branch_keyword_bu_num = {'Esher' : 1, 'Dorchester' : 2}\n",
        "branch_keyword_bu_num = {\n",
        "'Peterborough': 103,\n",
        "'Gillingham': 105,\n",
        "'Dorking': 107,\n",
        "'St Ives': 108,\n",
        "'Brighton': 114,\n",
        "'Brent Cross': 119,\n",
        "'Dorchester': 120,\n",
        "'Esher': 121,\n",
        "'Hall Green': 122,\n",
        "'Whetstone': 124,\n",
        "'Coulsdon': 129,\n",
        "'New Malden': 131,\n",
        "'Allington Park': 137,\n",
        "'Bury St Edmunds': 140,\n",
        "'Blaby': 141,\n",
        "'Marlow': 146,\n",
        "'Kingsthorpe': 148,\n",
        "'East Sheen': 149,\n",
        "'Four Oaks': 150,\n",
        "'Westbury Park': 151,\n",
        "'Leighton Buzzard': 154,\n",
        "'Stourbridge': 155,\n",
        "'Bromley': 158,\n",
        "'Birch Hill': 159,\n",
        "'Ramsgate': 160,\n",
        "'Huntingdon': 163,\n",
        "'Marlborough': 164,\n",
        "'Green Street Green': 165,\n",
        "'St Albans': 166,\n",
        "'Stevenage': 167,\n",
        "'Havant': 171,\n",
        "'John Barnes': 174,\n",
        "'Hertford': 175,\n",
        "'Beaconsfield': 177,\n",
        "'Enfield': 179,\n",
        "'Goldsworth Park': 181,\n",
        "'Sevenoaks': 182,\n",
        "'St Neots': 185,\n",
        "'Ruislip': 197,\n",
        "'Banstead': 202,\n",
        "'Ringwood': 203,\n",
        "'Welwyn Garden City': 204,\n",
        "'Ely': 205,\n",
        "'Thame': 206,\n",
        "'Chichester': 208,\n",
        "'Southend': 213,\n",
        "'Henley': 214,\n",
        "'Finchley': 215,\n",
        "'Godalming': 216,\n",
        "'Monmouth': 217,\n",
        "'Reading': 218,\n",
        "'Cirencester': 220,\n",
        "'Berkhamsted': 223,\n",
        "'Putney': 225,\n",
        "'Salisbury': 226,\n",
        "'Billericay': 229,\n",
        "'Horley': 233,\n",
        "'Okehampton': 234,\n",
        "'Waterlooville': 239,\n",
        "'Biggin Hill': 240,\n",
        "'Banstead': 324,\n",
        "'Horsham New': 580,\n",
        "'Heathfield': 595,\n",
        "'Cambridge': 651,\n",
        "'Hailsham': 653,\n",
        "'Hythe': 654,\n",
        "'Paddock Wood': 655,\n",
        "'Saltash': 656,\n",
        "'Sidmouth': 657,\n",
        "'Sudbury': 658,\n",
        "'Thatcham': 659,\n",
        "'Worcester Park': 661,\n",
        "'Wymondham': 662,\n",
        "'Cheltenham': 663,\n",
        "'Belgravia': 665,\n",
        "'Tonbridge': 667,\n",
        "'Chandlers Ford': 668,\n",
        "'Portishead': 669,\n",
        "'Romsey': 671,\n",
        "'Wandsworth': 673,\n",
        "'Newmarket': 674,\n",
        "'Sandbach': 680,\n",
        "'Fulham': 681,\n",
        "'Towcester': 682,\n",
        "'Abergavenny': 683,\n",
        "'Hitchin': 685,\n",
        "'Swaffham': 686,\n",
        "'Newport': 687,\n",
        "'Barry': 688,\n",
        "'Worthing': 689,\n",
        "'Otley': 691,\n",
        "'Farnham': 692,\n",
        "'Dartford': 693,\n",
        "'Sheffield': 695,\n",
        "'Wolverhampton': 696,\n",
        "'Willerby': 697,\n",
        "'Lichfield': 699,\n",
        "'Wilmslow': 711,\n",
        "'Lewes': 727,\n",
        "'East Grinstead': 741,\n",
        "'Buxton': 748,\n",
        "'St Katharine Docks': 753,\n",
        "'West Ealing': 764,\n",
        "'Hersham': 765,\n",
        "'Bishop s Stortford': 101,\n",
        "'Buckhurst Hill': 102,\n",
        "'Epsom': 104,\n",
        "'Longfield': 109,\n",
        "'Crowborough': 110,\n",
        "'Holloway Road': 112,\n",
        "'Milton Keynes': 115,\n",
        "'Dibden': 118,\n",
        "'Burgess Hill': 123,\n",
        "'Temple Fortune': 125,\n",
        "'Saffron Walden': 135,\n",
        "'Evington': 136,\n",
        "'Witney': 142,\n",
        "'Harrow Weald': 143,\n",
        "'Gosport': 152,\n",
        "'Wantage': 153,\n",
        "'Daventry': 156,\n",
        "'Weybridge': 157,\n",
        "'Winton': 161,\n",
        "'Andover': 168,\n",
        "'Southsea': 170,\n",
        "'Kings Road': 173,\n",
        "'Cobham': 176,\n",
        "'Caterham': 178,\n",
        "'Woodley': 180,\n",
        "'Harpenden': 183,\n",
        "'Caversham': 184,\n",
        "'Northwood': 186,\n",
        "'Richmond': 188,\n",
        "'West Byfleet': 189,\n",
        "'Sunningdale': 190,\n",
        "'Barnet': 191,\n",
        "'Chesham': 192,\n",
        "'Bath': 193,\n",
        "'Maidenhead': 194,\n",
        "'Kingston': 195,\n",
        "'Fleet': 196,\n",
        "'Yateley': 198,\n",
        "'Horsham': 200,\n",
        "'Tenterden': 201,\n",
        "'Bloomsbury': 207,\n",
        "'Petersfield': 209,\n",
        "'Stroud': 210,\n",
        "'Abingdon': 211,\n",
        "'Beckenham': 212,\n",
        "'South Harrow': 219,\n",
        "'Wokingham': 221,\n",
        "'Norwich': 222,\n",
        "'Bromley South': 224,\n",
        "'Newark': 227,\n",
        "'Gloucester Road': 230,\n",
        "'South Woodford': 231,\n",
        "'Surbiton': 232,\n",
        "'Staines': 235,\n",
        "'Marylebone': 236,\n",
        "'Great Malvern': 237,\n",
        "'Twyford': 238,\n",
        "'Byres Road': 308,\n",
        "'Weston Super Mare': 309,\n",
        "'Wellington': 315,\n",
        "'Ashbourne': 316,\n",
        "'Storrington': 317,\n",
        "'Menai Bridge': 318,\n",
        "'Melksham': 319,\n",
        "'Colchester': 455,\n",
        "'JL Foodhall Oxford Street': 456,\n",
        "'Pontprennau': 457,\n",
        "'Crewkerne': 458,\n",
        "'Kenilworth': 460,\n",
        "'Eldon Square': 461,\n",
        "'Westfield London': 462,\n",
        "'Winchester': 463,\n",
        "'Alcester': 474,\n",
        "'Bridport': 475,\n",
        "'Caldicot': 476,\n",
        "'Croydon': 477,\n",
        "'Haslemere': 478,\n",
        "'Headington': 479,\n",
        "'Holsworthy': 480,\n",
        "'Leigh On Sea': 481,\n",
        "'Ponteland': 482,\n",
        "'Saxmundham': 483,\n",
        "'Stamford': 484,\n",
        "'Torquay': 485,\n",
        "'Upminster': 486,\n",
        "'Lutterworth': 487,\n",
        "'Clerkenwell': 492,\n",
        "'JL Foodhall Bluewater': 493,\n",
        "'Altrincham': 494,\n",
        "'Frimley': 652,\n",
        "'Twickenham': 660,\n",
        "'Canary Wharf': 664,\n",
        "'Mill Hill': 670,\n",
        "'Droitwich': 672,\n",
        "'Wallingford': 675,\n",
        "'Newbury': 676,\n",
        "'Sanderstead': 677,\n",
        "'Kensington': 678,\n",
        "'Harrogate': 684,\n",
        "'Rushden': 690,\n",
        "'Lincoln': 694,\n",
        "'Rickmansworth': 698,\n",
        "'Ashford': 705,\n",
        "'Cheadle Hulme': 710,\n",
        "'Balham': 719,\n",
        "'Southampton New': 720,\n",
        "'Ampthill': 722,\n",
        "'Durham': 730,\n",
        "'Barbican': 732,\n",
        "'Formby': 749,\n",
        "'Comely Bank': 750,\n",
        "'Christchurch': 754,\n",
        "'Bayswater': 756,\n",
        "'Eastbourne': 757,\n",
        "'Chiswick': 760,\n",
        "'Morningside': 761,\n",
        "'Parkstone': 766,\n",
        "'Clapham Junction': 767,\n",
        "'Edgware Road': 768,\n",
        "'Buckingham': 769,\n",
        "'Windsor New': 772,\n",
        "'Islington': 780,\n",
        "'Hexham': 782,\n",
        "'Harborne': 796,\n",
        "'Brackley': 797,\n",
        "'Lymington New': 798,\n",
        "'Sandhurst': 799,\n",
        "'Trinity Square': 833,\n",
        "'Clifton': 834,\n",
        "'Crouch End': 835,\n",
        "'Oxted': 838,\n",
        "'Enfield CFC': 199,\n",
        "'Greenford CFC': 259,\n",
        "'Evesham': 303,\n",
        "'York': 311,\n",
        "'Poynton': 312,\n",
        "'East Cowes': 313,\n",
        "'Wimbledon': 314,\n",
        "'Knutsford': 326,\n",
        "'Newton Mearns': 327,\n",
        "'Stratford City': 328,\n",
        "'Alton': 329,\n",
        "'St Saviour (Jersey)': 332,\n",
        "'Rohais (Guernsey)': 333,\n",
        "'St Helier (Jersey)': 334,\n",
        "'Admiral Park (Guernsey)': 335,\n",
        "'Red Houses (Jersey)': 336,\n",
        "'MOUNTSORREL': 403,\n",
        "'Gerrards Cross': 459,\n",
        "'Sevenoaks': 464,\n",
        "'Marlow': 465,\n",
        "'Cardiff Queen Street': 501,\n",
        "'Acton': 502,\n",
        "'Swindon': 504,\n",
        "'Littlehampton': 505,\n",
        "'Uckfield': 506,\n",
        "'Hereford': 507,\n",
        "'Malmesbury': 511,\n",
        "'Coulsdon DFC': 513,\n",
        "'Bagshot': 514,\n",
        "'Nailsea': 515,\n",
        "'Parsons Green': 516,\n",
        "'Egham': 519,\n",
        "'Jesmond': 520,\n",
        "'Enfield Chase': 521,\n",
        "'Sutton Coldfield': 522,\n",
        "'Chippenham': 523,\n",
        "'West Hampstead': 524,\n",
        "'Shrewsbury': 525,\n",
        "'Tottenham Court Road': 526,\n",
        "'Dorking': 527,\n",
        "'Wimbledon Hill': 528,\n",
        "'Hawkhurst': 529,\n",
        "'Fulham Palace Road': 530,\n",
        "'Peterborough': 531,\n",
        "'Canterbury': 533,\n",
        "'Sceptre (Watford)': 534,\n",
        "'Kensington Gardens': 535,\n",
        "'Camden': 536,\n",
        "'Addlestone': 542,\n",
        "'Fitzroy Street': 552,\n",
        "'Teignmouth': 554,\n",
        "'Hornchurch': 555,\n",
        "'Edenbridge': 556,\n",
        "'Keynsham': 557,\n",
        "'Spinningfields': 558,\n",
        "'Cheam': 559,\n",
        "'Alderley Edge': 560,\n",
        "'Walton-on-Thames': 562,\n",
        "'Locks Heath': 563,\n",
        "'Burgh Heath': 567,\n",
        "'Petts Wood': 568,\n",
        "'Portman Square': 569,\n",
        "'Burnt Common': 571,\n",
        "'Walbrook': 573,\n",
        "'Leeds': 574,\n",
        "'Broxbourne': 575,\n",
        "'Amersham': 578,\n",
        "'Bayswater Temp': 579,\n",
        "'Oxford Botley Road': 581,\n",
        "'BASINGSTOKE': 582,\n",
        "'Old Brompton Road': 583,\n",
        "'Hazlemere': 584,\n",
        "'Ealing': 586,\n",
        "'West Kensington': 587,\n",
        "'Palmers Green': 588,\n",
        "'Guildford': 589,\n",
        "'Kings Cross': 590,\n",
        "'Wollaton': 591,\n",
        "'Rustington': 596,\n",
        "'BATTERSEA NINE ELMS': 598,\n",
        "'UTTOXETER': 599,\n",
        "'High Holborn': 601,\n",
        "'Alderley Old': 602,\n",
        "'Sherborne': 604,\n",
        "'Hove': 605,\n",
        "'Leek': 606,\n",
        "'High Wycombe': 607,\n",
        "'Hampton': 612,\n",
        "'Pimlico': 614,\n",
        "'Foregate Street': 615,\n",
        "'Clapham Common': 616,\n",
        "'Kings Cross Station': 619,\n",
        "'Stirling': 620,\n",
        "'North Walsham': 622,\n",
        "'Aylesbury': 625,\n",
        "'Milngavie': 630,\n",
        "'Ipswich': 632,\n",
        "'Manchester Piccadilly': 636,\n",
        "'Highbury Corner': 637,\n",
        "'Muswell Hill': 639,\n",
        "'Knightsbridge': 641,\n",
        "'Solihull': 642,\n",
        "'Sidcup': 643,\n",
        "'Notting Hill Gate': 644,\n",
        "'Truro': 648,\n",
        "'Worcester': 700,\n",
        "'Warminster': 701,\n",
        "'Exeter': 702,\n",
        "'South Bank Tower': 703,\n",
        "'Bracknell': 706,\n",
        "'Stratford Upon Avon': 708,\n",
        "'Walton-le-Dale': 721,\n",
        "'Bedford': 725,\n",
        "'Wootton': 726,\n",
        "'Market Harborough': 728,\n",
        "'Wells': 729,\n",
        "'Poundbury': 733,\n",
        "'Cowbridge': 735,\n",
        "'ROEHAMPTON': 736,\n",
        "'Battersea': 737,\n",
        "'Bagshot Road': 738,\n",
        "'Tubs Hill': 739,\n",
        "'Greenwich': 740,\n",
        "'Colmore Row (Birmingham)': 742,\n",
        "'Ipswich (Corn Exchange)': 743,\n",
        "'Kings Hill': 744,\n",
        "'Chipping Sodbury': 751,\n",
        "'Oakgrove': 752,\n",
        "'Dorking': 755,\n",
        "'Oundle': 758,\n",
        "'Northwich': 759,\n",
        "'Helensburgh': 771,\n",
        "'Monument': 773,\n",
        "'Little Waitrose at John Lewis Watford': 781,\n",
        "'Victoria Street': 783,\n",
        "'Vauxhall': 789,\n",
        "'Horley - Brighton Road': 802,\n",
        "'Wimborne': 805,\n",
        "'Headington - London Road': 806,\n",
        "'Guildford Worplesdon Road': 808,\n",
        "'Little Waitrose John Lewis Southampton': 815,\n",
        "'East Putney': 820,\n",
        "'Meanwood': 828,\n",
        "'Chester': 842,\n",
        "'Raynes Park': 846,\n",
        "'Oadby': 847,\n",
        "'Leatherhead': 859,\n",
        "'Victoria Bressenden Place': 860,\n",
        "'SKY (OSTERLEY)': 865,\n",
        "'Faringdon': 871,\n",
        "'Haywards Heath': 873,\n",
        "'Banbury': 874,\n",
        "'Finchley Central': 876,\n",
        "'Bromsgrove': 877,\n",
        "'Winchmore Hill': 878,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MAOuI7dYN9Nf"
      },
      "outputs": [],
      "source": [
        "# England = ['Avon', 'Bedfordshire', 'Berkshire', 'Buckinghamshire', 'Cambridgeshire', 'Cheshire', 'Cleveland',\n",
        "#            'Cornwall', 'Cumbria', 'Derbyshire', 'Devon', 'Dorset', 'Durham', 'East-Sussex', 'Essex', 'Gloucestershire',\n",
        "#            'Hampshire', 'Herefordshire', 'Hertfordshire', 'Isle-of-Wight', 'Kent', 'Lancashire', 'Leice stershire',\n",
        "#            'Lincolnshire', 'London', 'Merseyside',\n",
        "#            'Middlesex', 'Norfolk', 'Northamptonshire', 'Northumberland', 'North-Humberside', 'North-Yorkshire',\n",
        "#            'Nottinghamshire', 'Oxfordshire', 'Rutland', 'Shropshire', 'Somerset', 'South-Humberside', 'South-Yorkshire',\n",
        "#            'Staffordshire', 'Suffolk', 'Surrey', 'Tyne-and-Wear', 'Warwickshire', 'West-Midlands', 'West-Sussex',\n",
        "#            'West-Yorkshire', 'Wiltshire', 'Worcestershire']\n",
        "England = ['London']\n",
        "Wales = ['Clwyd', 'Dyfed', 'Gwent', 'Gwynedd', 'Mid-Glamorgan',\n",
        "         'Powys', 'South-Glamorgan', 'West-Glamorgan']\n",
        "# Wales = ['South-Glamorgan']\n",
        "Scotland = ['Aberdeenshire', 'Angus', 'Argyll', 'Ayrshire', 'Banffshire', 'Berwickshire', 'Bute', 'Caithness',\n",
        "            'Clackmannanshire', 'Dumfriesshire', 'Dunbartonshire', 'East-Lothian', 'Fife', 'Inverness-shire',\n",
        "            'Kincardineshire', 'Kinross-shire',\n",
        "            'Kirkcudbrightshire', 'Lanarkshire', 'Midlothian', 'Moray', 'Nairnshire', 'Orkney', 'Peeblesshire',\n",
        "            'Perthshire', 'Renfrewshire', 'Ross-shire', 'Roxburghshire', 'Selkirkshire', 'Shetland', 'Stirlingshire',\n",
        "            'Sutherland', 'West Lothian', 'Wigtownshire']\n",
        "NorthernIreland = ['Antrim', 'Armagh', 'Down',\n",
        "                   'Fermanagh', 'Londonderry', 'Tyrone']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wqosM5ySN9Ng"
      },
      "outputs": [],
      "source": [
        "# branch_keyword = ['Abergavenny', 'Alderley Edge', \"Eastbourne\", \"Edenbridge\", \"Pontprennau\"]\n",
        "# branch_keyword = ['Abingdon', 'Canary Wharf']\n",
        "# all_branch_keyword = ['Yateley', 'Canary Wharf', 'Workingham', 'Firmley']\n",
        "all_branch_keyword = list(branch_keyword_bu_num.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q_bhawhRNxnx"
      },
      "outputs": [],
      "source": [
        "branch_keyword = all_branch_keyword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bcX9WfgAN9Ng"
      },
      "outputs": [],
      "source": [
        "# countries = [England, Wales, Scotland, NorthernIreland]\n",
        "countries = [England]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yPnMjBxmN9Ng"
      },
      "outputs": [],
      "source": [
        "final = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HjbT2oHwGenn"
      },
      "outputs": [],
      "source": [
        "# final_prod = pd.DataFrame()\n",
        "status_val = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HlTvy7QmN9Nh"
      },
      "outputs": [],
      "source": [
        "def googleNewsByStreet():\n",
        "    data = pd.DataFrame()\n",
        "    for branch in branch_keyword:\n",
        "        for keyword in keywords:\n",
        "            news = GoogleNews()\n",
        "            news.set_period('1d')\n",
        "            news.get_news(branch + ' ' + keyword)\n",
        "            results = news.result()\n",
        "            df = pd.DataFrame.from_dict(results)\n",
        "            df['keyword'] = keyword\n",
        "            df['branch'] = branch\n",
        "            df['bu_num'] = branch_keyword_bu_num[branch]\n",
        "            print(df)\n",
        "            df.head(5)\n",
        "            data = pd.concat([data, df], ignore_index=True)\n",
        "\n",
        "    # print the dataframe\n",
        "    if len(data.columns) > 4:\n",
        "      data = data.drop(columns=[\"img\", \"site\"])\n",
        "      final.append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kitk6GatN9Nj"
      },
      "outputs": [],
      "source": [
        "def _removeNonAscii(s):\n",
        "    return \"\".join(i for i in s if ord(i) < 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nQ38jsUDN9Nj"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = text.replace('(ap)', '')\n",
        "    text = re.sub(r\"\\'s\", \" is \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r\"\\\\\", \"\", text)\n",
        "    text = re.sub(r\"\\'\", \"\", text)\n",
        "    text = re.sub(r\"\\\"\", \"\", text)\n",
        "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
        "    text = _removeNonAscii(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZDg2YrQ7N9Nk"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(word_tokens):\n",
        "    filtered_sentence = []\n",
        "    stop_words = stopwords.words('english')\n",
        "    specific_words_list = ['char', 'u', 'hindustan', 'doj', 'washington']\n",
        "    stop_words.extend(specific_words_list)\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "    return filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "C4HnnrsiN9Nk"
      },
      "outputs": [],
      "source": [
        "def lemmatize(x):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "n-lwf6tuN9Nk"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CASnFpj9N9Nk"
      },
      "outputs": [],
      "source": [
        "def tokenize(x):\n",
        "    return tokenizer.tokenize(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdQ_hWw_N9Nl"
      },
      "outputs": [],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "amt8cwhNjU_9"
      },
      "outputs": [],
      "source": [
        "def sentiment_analysis(prod):\n",
        "    prod['combined_text'] = prod['title'].map(str)\n",
        "\n",
        "    # applying all of these functions to the our dataframe\n",
        "    prod['combined_text'] = prod['combined_text'].map(clean_text)\n",
        "    prod['tokens'] = prod['combined_text'].map(tokenize)\n",
        "    prod['tokens'] = prod['tokens'].map(remove_stopwords)\n",
        "    prod['lems'] = prod['tokens'].map(lemmatize)\n",
        "    sia = SIA()\n",
        "    results = []\n",
        "    for line in prod['lems']:\n",
        "        pol_score = sia.polarity_scores(line)\n",
        "        pol_score['lems'] = line\n",
        "        results.append(pol_score)\n",
        "    headlines_polarity = pd.DataFrame.from_records(results)\n",
        "    temp = []\n",
        "    # for line in prod['branch']:\n",
        "        # temp.append(line)\n",
        "    # headlines_polarity['branch'] = temp\n",
        "    headlines_polarity['label'] = 0\n",
        "    headlines_polarity.loc[headlines_polarity['compound'] > 0.2, 'label'] = 1\n",
        "    headlines_polarity.loc[headlines_polarity['compound'] < -0.2, 'label'] = -1\n",
        "    headlines_polarity['word_count'] = headlines_polarity['lems'].apply(lambda x: len(str(x).split()))\n",
        "    headlines_polarity.head()\n",
        "    # gk = headlines_polarity.groupby(['branch', 'label'])\n",
        "    # fk = headlines_polarity.groupby('branch')['compound'].mean()\n",
        "    # fk = fk.to_frame()\n",
        "    result = [prod, headlines_polarity]\n",
        "    headlines_polarity = headlines_polarity.rename_axis(index=None)\n",
        "    return pd.merge(prod, headlines_polarity, on=[\"lems\"], how=\"left\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "c2edY3CuN9Nl"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "def outsource_news():\n",
        "    googleNewsByStreet()\n",
        "    prod = pd.concat(final)\n",
        "    prod = prod.drop_duplicates('title', keep='first')\n",
        "    print(prod)\n",
        "    status_val.append(30)\n",
        "\n",
        "    final_prod = sentiment_analysis(prod)\n",
        "\n",
        "    # mail_data(final_prod) & upload_data_complete(final_prod)\n",
        "\n",
        "    final_prod = final_prod.replace(np.nan,'',regex=True)\n",
        "\n",
        "    # forecast_keywords = ['sale', 'sport', 'beverage', 'retail', 'vendor', 'market', 'morrisons', 'tesco', 'coles', 'business', 'shopping', 'weather',\n",
        "    #                      'parties', 'events', 'walmart']\n",
        "\n",
        "    second_keywords = ['bank holiday', 'heatwave', 'inflation', 'street party', 'rainfall', 'snow', 'retail', 'beverage', 'tesco', 'walmart', 'morrisons', 'weather',\n",
        "                       'brc', 'mothers day', 'new store launch', 'lidl', 'homebase', 'walmart', 'new tesco store', 'coles', 'supermarket', 'shoppers', 'store', 'grocery', 'strike', 'holiday'\n",
        "                       'shops', 'markets','holiday', 'lockdown','grocery sales', 'carnival', 'festival', 'party', \"sainsbury\", \"supply chain\", \"flood\", \"wendys\",\n",
        "                       'ocado', 'spencer', 'asda']\n",
        "\n",
        "    remove_keywords = ['accident', 'incident', 'injury', 'political', 'police', 'death', 'traffic', 'lord', 'war', 'actor', 'movie', 'star', 'lord', 'sex', 'gay',\n",
        "                       'fight', 'crash', 'life', 'weapons', 'dating', 'radio', 'tv', 'guinness', 'husband', 'fashion', 'attack']\n",
        "\n",
        "    store_keywords = ['opens', 'closes', 'closed', 'opened', 'open', 'close',\n",
        "                      'shut', 'confining', 'unopen', 'opening',\n",
        "                      'close down', 'closing', 'shut down', 'conclude', 'ending', 'shutdown', 'closedown',\n",
        "                      'closure', 'temporary', 'extended', 'shutting', 'launch', 'shuts', 'closures']\n",
        "\n",
        "    store_remove_keywords = ['ftse', 'pubs', 'pub', 'life', 'stocks', 'earnings', 'dining', 'restaurants', 'stock', 'rocket', 'fashion', 'restaurant',\n",
        "                             'letter', 'bills', 'investment', 'childrenswear', 'blizzard', 'infamous', 'qualifying', 'sports', 'bar', 'cafe',\n",
        "                             'technology', 'dental', 'boobs', 'school', 'flixbus', 'allegations', 'pharmacy', 'attack', 'driver', 'fitness', 'students',\n",
        "                             'charities']\n",
        "\n",
        "    competitor_keywords = ['tesco', 'wendys', 'lidl', 'sainsburys', 'sainsbury', 'aldi', 'morrisons', 'spencer', 'asda', 'supermarket',\n",
        "                            'co', 'ocado', 'sparks', 'b&m', 'iceland', 'waitrose']\n",
        "\n",
        "    print(final_prod)\n",
        "\n",
        "    for index, row in final_prod.iterrows():\n",
        "      if (len(np.intersect1d(row['tokens'], store_keywords)) == 0):\n",
        "        # if(len(np.intersect1d(row['tokens'], competitor_keywords)) == 0):\n",
        "        final_prod.drop(index=index, axis=0, inplace=True)\n",
        "      else:\n",
        "        if(len(np.intersect1d(row['tokens'], competitor_keywords)) == 0):\n",
        "          final_prod.drop(index=index, axis=0, inplace=True)\n",
        "\n",
        "    for index, row in final_prod.iterrows():\n",
        "      for value in row['tokens']:\n",
        "        val = value.capitalize()\n",
        "        try:\n",
        "            final_prod.at[index,'bu_num'] = branch_keyword_bu_num[val]\n",
        "            final_prod.at[index,'branch'] = val\n",
        "        except:\n",
        "            n = 0\n",
        "\n",
        "    final_prod = final_prod.drop_duplicates('title', keep='first')\n",
        "    final_prod = final_prod.drop_duplicates('lems', keep='first')\n",
        "    final_prod = final_prod.drop_duplicates('tokens', keep='first')\n",
        "\n",
        "    final_prod['title'] = final_prod['title'].astype(str)\n",
        "\n",
        "    final_prod['competitor_evt_indchar'] = ['Yes' if(len(np.intersect1d(x,competitor_keywords)) > 0) else 'No' for x in final_prod['tokens']]\n",
        "\n",
        "    counter_guid = int(date.today().strftime(\"%Y%m%d\"))\n",
        "    final_prod['efsevt_guid'] = [(counter_guid*1000)+i for i in range(len(final_prod))]\n",
        "\n",
        "    print(final_prod.dtypes)\n",
        "    print(final_prod_events.dtypes)\n",
        "\n",
        "    foriegn_key = []\n",
        "\n",
        "    for index, row in final_prod.iterrows():\n",
        "      flag = False\n",
        "      for index_event, row_event in final_prod_events.iterrows():\n",
        "        if(row['keyword'] != '' ):\n",
        "          if(row['keyword'] in row_event['NAME']):\n",
        "            print(row['keyword'],row_event['NAME'])\n",
        "            foriegn_key.append(row_event['GUID'])\n",
        "            flag = True\n",
        "      if(flag == False):\n",
        "        foriegn_key.append(0)\n",
        "\n",
        "    print(foriegn_key)\n",
        "\n",
        "    final_prod['guid'] = [(counter_guid*2000)+i for i in range(len(final_prod))]\n",
        "    final_prod['fixed_annual_ind'] = 'n'\n",
        "    final_prod['perm_env_ind'] = 'n'\n",
        "    final_prod['cancelled_ind'] = 'n'\n",
        "    final_prod['create_user'] = ''\n",
        "    final_prod['update_user'] = ''\n",
        "    final_prod['perm_env_ind'] = 'n'\n",
        "    final_prod['crt_timestamp'] = date.today()\n",
        "    final_prod['upd_timestamp'] = date.today()\n",
        "\n",
        "\n",
        "    final_prod.rename(columns = {'link':'source_of_event'}, inplace = True)\n",
        "    final_prod[[\"datetime\"]] = final_prod[[\"datetime\"]].astype(str)\n",
        "    final_prod.columns = final_prod.columns.str.upper()\n",
        "\n",
        "    final_prod.to_csv('Events.csv', mode='a', index=False, header=False)\n",
        "    return final_prod\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBR08rg-jxHV"
      },
      "outputs": [],
      "source": [
        "!pip install geopy\n",
        "!pip install pgeocode\n",
        "\n",
        "from geopy.geocoders import Photon, GoogleV3, Nominatim\n",
        "import pgeocode\n",
        "from math import cos, asin, sqrt, pi\n",
        "\n",
        "def distance(lat1, lon1, lat2, lon2):\n",
        "    p = pi/180\n",
        "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
        "    return round(12742 * asin(sqrt(a)),2)\n",
        "\n",
        "\n",
        "def place_distance(string1,string2):\n",
        "    try:\n",
        "      geolocator_addr = Nominatim(user_agent=\"http\")\n",
        "      # place = \"Lidl,Bath\"\n",
        "      # place_2 = \"Waitrose,Bath\"\n",
        "      place = string1 + \",\" + string2\n",
        "      place_2 = \"Waitrose,\" + string2\n",
        "      # location = geolocator.geocode(place)\n",
        "      pin = geolocator_addr.geocode(place)\n",
        "      pin_2 = geolocator_addr.geocode(place_2)\n",
        "      # print(location)\n",
        "      print(pin)\n",
        "      print(pin_2)\n",
        "      print(pin.raw['lat'],pin.raw['lon'],pin_2.raw['lat'],pin_2.raw['lon'])\n",
        "    except:\n",
        "      return 'N/A'\n",
        "\n",
        "    return distance(float(pin.raw['lat']),float(pin.raw['lon']),float(pin_2.raw['lat']),float(pin_2.raw['lon']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "road = []"
      ],
      "metadata": {
        "id": "wRLgOI_63mMN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests lxml\n",
        "!pip install beautifulsoup4\n",
        "!pip install geopy\n",
        "!pip install pgeocode\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Photon, GoogleV3, Nominatim\n",
        "import pgeocode\n",
        "from math import cos, asin, sqrt, pi\n",
        "import re\n",
        "import math\n",
        "\n",
        "def distance_infrastructure(lat1, lon1, lat2, lon2):\n",
        "    p = pi/180\n",
        "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
        "    return round(12742 * asin(sqrt(a)),2)\n",
        "\n",
        "\n",
        "def place_distance_infrastructure(string2, work):\n",
        "    temp = []\n",
        "    try:\n",
        "      route_df = pd.read_csv(\"Branch_Lat_Lon.csv\")\n",
        "      geolocator_addr = Nominatim(user_agent=\"http\")\n",
        "      place_2 = string2 + \", London, UK\"\n",
        "      pin_2 = geolocator_addr.geocode(place_2)\n",
        "      print(pin_2)\n",
        "      print(pin_2.raw['lat'],pin_2.raw['lon'])\n",
        "      actual = 999999\n",
        "      if(pin_2 != 'None'):\n",
        "        for index, row in route_df.iterrows():\n",
        "          dif = distance_infrastructure(float(pin_2.raw['lat']),float(pin_2.raw['lon']),row[\"lat\"],row[\"lon\"])\n",
        "          if( dif < 20):\n",
        "            if(actual > dif):\n",
        "              actual = dif\n",
        "              branch_name = row[\"branch\"]\n",
        "        if(actual <= 2):\n",
        "          print(actual)\n",
        "          temp.append(branch_name)\n",
        "          print(branch_name)\n",
        "          print(\"Yes\")\n",
        "          road.append([ string2 + \" \" + work, branch_name, \"\", \"\", \"\", \"\", actual])\n",
        "          return branch_name\n",
        "        print(actual)\n",
        "    except:\n",
        "      n = 0\n",
        "    return None\n",
        "\n",
        "\n",
        "def infrastructure():\n",
        "  url = \"https://tfl.gov.uk/traffic/status/?Input=&lineIds=&dateTypeSelect=Future%20date&direction=&startDate=2023-06-27T00%3A00%3A00&endDate=2023-06-27T23%3A59%3A59&lat=51.50721740722656&lng=-0.12758620083332062&placeType=stoppoint&input=London%2C%20UK\"\n",
        "  # url = \"https://tfl.gov.uk/traffic/status/?Input=England%2C%20UK&lineIds=&dateTypeSelect=Future%20date&direction=&startDate=2023-06-23T00%3A00%3A00&endDate=2023-06-23T23%3A59%3A59&lat=52.35551834106445&lng=-1.1743197441101074&placeType=placeextra&input=london%2C%20uk\"\n",
        "  # Send a GET request to the website\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "  ele = soup.select('div[class^=\\\"road-disruption\\\"]')\n",
        "  # print(ele[0].text)\n",
        "  street = []\n",
        "  works = []\n",
        "  for element in ele:\n",
        "      h2_tags = element.select('h4')\n",
        "      p_tags = element.select('p[class^=\\\"topmargin\\\"]')\n",
        "      date_tags = element.select('p[class^=\\\"highlight dates\\\"]')\n",
        "      print(date_tags[0].text.strip(\"\\n\\n\").split(\"\\n\"))\n",
        "      for h2_tag,p_tag in zip(h2_tags,p_tags):\n",
        "          # print(h2_tag.text.strip().split(\" \"))\n",
        "          if(\"Works\" in p_tag.text):\n",
        "            arr = h2_tag.text.strip().split(\" \")\n",
        "            word = \"\"\n",
        "            for i in range(1,len(arr)-1):\n",
        "              if('(' not in arr[i]):\n",
        "                word = word + \" \" + arr[i]\n",
        "            # word = word + \" \" +arr[-1]\n",
        "            street.append(word)\n",
        "            works.append(p_tag.text)\n",
        "\n",
        "  street = list(set(street))\n",
        "  print(len(street))\n",
        "  # print(street)\n",
        "\n",
        "  for i,j in zip(street,works):\n",
        "    place_distance_infrastructure(i,j)\n",
        "\n",
        "  return road\n",
        "\n",
        "\n",
        "# place_distance_infrastructure(\"Lea Gate, Blackpool Road, Preston\", \"\")\n"
      ],
      "metadata": {
        "id": "32NQzwC3sQUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty-html-table\n",
        "!pip install pyshorteners\n",
        "!pip install xlsxwriter\n",
        "\n",
        "import smtplib, ssl\n",
        "from smtplib import SMTP\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import pandas as pd\n",
        "from pretty_html_table import build_table\n",
        "from pyshorteners import Shortener\n",
        "from io import StringIO, BytesIO\n",
        "from email.mime.application import MIMEApplication\n",
        "from datetime import date, timedelta\n",
        "import xlsxwriter\n",
        "\n",
        "def mail_data(final_prod):\n",
        "  port = 465  # For SSL\n",
        "  context = ssl.create_default_context()\n",
        "  mail_df = pd.DataFrame()\n",
        "\n",
        "  mail_df[\"TITLE\"] = final_prod[\"TITLE\"]\n",
        "  mail_df[\"BRANCH\"] = final_prod[\"BRANCH\"]\n",
        "  mail_df[\"SOURCE\"] = final_prod[\"MEDIA\"]\n",
        "  mail_df[\"DATETIME\"] = final_prod[\"DATETIME\"]\n",
        "  mail_df[\"BRANCH_NUM\"] = final_prod[\"BU_NUM\"]\n",
        "\n",
        "  distance_arr = []\n",
        "  for index, row in final_prod.iterrows():\n",
        "    if(row[\"KEYWORD\"] and row[\"BRANCH\"]):\n",
        "        distance_arr.append(place_distance(row[\"KEYWORD\"], row[\"BRANCH\"]))\n",
        "  mail_df[\"DISTANCE IN MILES\"] = distance_arr\n",
        "\n",
        "  urls = []\n",
        "  for index,row in final_prod.iterrows():\n",
        "      x = Shortener().tinyurl.short(row[\"SOURCE_OF_EVENT\"])\n",
        "      urls.append(x)\n",
        "\n",
        "  mail_df[\"LINK\"] = urls\n",
        "\n",
        "  for index, row in mail_df.iterrows():\n",
        "    if(row[\"DISTANCE IN MILES\"] != 'N/A'):\n",
        "      if(row[\"DISTANCE IN MILES\"] > 25):\n",
        "          mail_df.drop(index=index, axis=0, inplace=True)\n",
        "\n",
        "  road = infrastructure()\n",
        "  print(road)\n",
        "  road_df = pd.DataFrame(road, columns = [\"TITLE\", \"BRANCH\", \"SOURCE\", \"DATETIME\", \"BRANCH_NUM\", \"DISTANCE IN MILES\", \"LINK\"])\n",
        "  road_df = road_df.drop_duplicates(\"BRANCH\", keep=\"first\")\n",
        "\n",
        "  html_table = mail_df.to_html(index=False, classes='example-table')\n",
        "  road_table = road_df.to_html(index=False, classes='example-table')\n",
        "\n",
        "  # text = f\"Hello Alex and Olga,\\n Herewith attaching the events captured for all the competitors (core event types) including all the branches from \"+ (date.today() - timedelta(days = 1)).strftime(\"%d-%m-%Y\") +\" to \" + date.today().strftime(\"%d-%m-%Y\") + \" which are auto-generated from the script.\\n\\n\\nThanks And Regards,\\nSashank L\\n\\n\\n\"\n",
        "  text = f\"Hello Alex and Olga,\\n Herewith attaching the events captured for all the competitors (core event types) including all the branches from 23-06-2023\" + \" to \" + \"24-06-2023 which are auto-generated from the script.\\n\\n\\nThanks And Regards,\\nSashank L\\n\\n\\n\"\n",
        "\n",
        "  print(mail_df)\n",
        "\n",
        "  html_table = html_table.replace('<th>', '<th style=\"padding: 10px 90px 10px 90px;\">', 1)\n",
        "  road_table = road_table.replace('<th>', '<th style=\"padding: 10px 80px 10px 80px;\">', 1)\n",
        "\n",
        "  if(mail_df.empty):\n",
        "    html_table\n",
        "\n",
        "# HTML Styling\n",
        "  html = f'''\n",
        "<html>\n",
        "<head>\n",
        "    <style>\n",
        "        table.example-table th{{\n",
        "              padding: 10px;\n",
        "              text-align: center;\n",
        "              background-color: #FFFFFF;\n",
        "              font-weight: bold;\n",
        "              font-size: 14px;\n",
        "              width: 400px;\n",
        "          }}\n",
        "\n",
        "          table.example-table th:first-child {{\n",
        "            padding: 20px 100px 20px 100px; /* Set the desired width for the fourth column */\n",
        "          }}\n",
        "\n",
        "          table.example-table td {{\n",
        "            padding: 5px;\n",
        "            color: black;\n",
        "            font-size: 12px;\n",
        "            width: 400px;\n",
        "            font-family: Century Gothic, sans-serif;\n",
        "          }}\n",
        "\n",
        "        /* Add custom styles here */\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "      <pre>{text}</pre>\n",
        "        {html_table}\n",
        "        <br/>\n",
        "        <br/>\n",
        "        {road_table}\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "  part1 = MIMEText(html, 'html')\n",
        "  msg = MIMEMultipart(\"alternative\")\n",
        "  msg['Subject'] = \"Automated Event Capturing Model\"\n",
        "  # recipients = ['amit.kumbhar@johnlewis.co.uk', 'mitali.patel@johnlewis.co.uk', \"alex.nicola@waitrose.co.uk\", \"olga.jakobsone@waitrose.co.uk\"]\n",
        "  recipients = ['sashank99narayan@gmail.com']\n",
        "  msg['To'] = \", \".join(recipients)\n",
        "  msg.attach(part1)\n",
        "\n",
        "  file_name = date.today().strftime(\"%d-%m-%Y\") + \"_Events.xlsx\"\n",
        "  # output = io.BytesIO()\n",
        "  textStream = BytesIO()\n",
        "  writer = pd.ExcelWriter(textStream, engine='xlsxwriter')\n",
        "  mail_df.to_excel(writer,sheet_name=\"Competitor Events\",index=False)\n",
        "  road_df.to_excel(writer,sheet_name=\"Road Closure Events\",index=False)\n",
        "  textStream.seek(0)\n",
        "\n",
        "  attachment = MIMEApplication(textStream.read(), name= file_name)\n",
        "  attachment['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(file_name)\n",
        "  msg.attach(attachment)\n",
        "\n",
        "  with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n",
        "      print(\"IN\")\n",
        "      server.login(\"sashank99narayan@gmail.com\", \"tauvozkdtqbkqycf\")\n",
        "      server.sendmail(\"sashank99narayan@gmail.com\", recipients, msg.as_string())\n"
      ],
      "metadata": {
        "id": "cFBGJI7DBYMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3FYrkChWAUo"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  final_prod = outsource_news()\n",
        "  mail_data(final_prod)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
